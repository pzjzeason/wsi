{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入包及预处理语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "# 修改plt默认的样式，包括背景、网格线等\n",
    "sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n",
    "# 支持中文标题、标签 for mac\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更改k-means源码后立即升效\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP预处理\n",
    "import hanlp\n",
    "tokenizer = hanlp.load('LARGE_ALBERT_BASE')\n",
    "tagger = hanlp.load(hanlp.pretrained.pos.CTB9_POS_ALBERT_BASE)\n",
    "recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)\n",
    "syntactic_parser = hanlp.load(hanlp.pretrained.dep.CTB7_BIAFFINE_DEP_ZH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/CLP2010/parsed.json','r') as f:\n",
    "    corpus_labeled_parsed = json.load(f)\n",
    "with open('./data/CLP2010/flat.json','r') as f:\n",
    "    corpus_labeled_flat = json.load(f)\n",
    "with open('./data/CLP2010/meta.json','r') as f:\n",
    "    corpus_labeled_meta = json.load(f)\n",
    "with open('./data/CLP2010/corpus.json','r') as f:\n",
    "    corpus_labeled = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# 语料及预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## help function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_path(path, *args):\n",
    "    \"\"\"检查目录是否存在，不存在就创建\"\"\"\n",
    "    path = path % args\n",
    "    parent_path = os.path.split(path)[0]\n",
    "    if not os.path.exists(parent_path):\n",
    "        os.makedirs(parent_path)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(root, bool_func=False):\n",
    "    \"\"\"获得某个文件夹下的子文件，可以传递一个bool_func函数，判断子文件是否保留\"\"\"\n",
    "    if not bool_func:\n",
    "        bool_func = lambda x: True\n",
    "    files = []\n",
    "    for file in os.listdir(root):\n",
    "        if bool_func(file):\n",
    "            files.append(os.path.join(root,file)) \n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词&依存句法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_long_sentence(sentence, target):\n",
    "    \"\"\"长句取包含目标词的短句，也就是标点之前包含目标词的部分\"\"\"\n",
    "    target_start = sentence.find(target)\n",
    "    left_start = right_end = 0\n",
    "    for i in range(target_start-1,-1,-1):\n",
    "        if sentence[i] in [',','.','，','。',';','；']:\n",
    "            left_start = i\n",
    "            break\n",
    "    for i in range(target_start,len(sentence),1):\n",
    "        if sentence[i] in [',','.','，','。',';','；'] or i==len(sentence)-1:\n",
    "            right_end = i\n",
    "            break\n",
    "    return sentence[left_start+1:right_end+1] if left_start!=0 else sentence[left_start:right_end+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(sentence, target=False):\n",
    "    if len(sentence)>120:\n",
    "        sentence = process_long_sentence(sentence,target)\n",
    "    parse_result = {}\n",
    "    is_target_split = False\n",
    "    tokens = tokenizer(sentence)\n",
    "    if target and target not in tokens:\n",
    "        tokens_new = []\n",
    "        for index, tok in enumerate(tokens):\n",
    "            if target in tok:\n",
    "                start = tok.find(target)\n",
    "                tokens_new+=([tok[:start],target] if start>0 else [target,tok[start+len(target):]])\n",
    "            else:\n",
    "                tokens_new.append(tok)\n",
    "        tokens = tokens_new\n",
    "        is_target_split = True\n",
    "    pos = tagger(tokens)\n",
    "    dp = syntactic_parser(list(zip(tokens,pos)))\n",
    "    ne = recognizer(list(sentence))\n",
    "    parse_result['tokens'] = tokens\n",
    "    parse_result['pos'] = pos\n",
    "    parse_result['ne'] = ne\n",
    "    parse_result['dp'] = [{'index':d['id']-1,'token':d['form'], 'pos':d['cpos'],'relate': d['deprel'], 'head': d['head']-1} for d in dp]\n",
    "    parse_result['sentence'] = sentence\n",
    "    parse_result['is_target_split'] = is_target_split\n",
    "    if target:\n",
    "        parse_result['target'] = target\n",
    "        try:\n",
    "            parse_result['target_index'] = tokens.index(target)\n",
    "        except:\n",
    "            # 由于分词原因，目标词没有被切分出来，而是和其他字词成为了一个复合词，例如“标兵区”，而没切成“标兵”、“区”\n",
    "            parse_result['target_index'] = -1 \n",
    "    return parse_result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "with open ('./data/CLP2010/AnswerData/暗淡.xml') as f:\n",
    "    soup = BeautifulSoup(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_labeled = {}\n",
    "for file in get_files('./data/CLP2010/AnswerData/'):\n",
    "    with open(file) as f:\n",
    "        soup = BeautifulSoup(f)\n",
    "    word = soup.lexelt['item']\n",
    "    res = {}\n",
    "    for sense in soup.select('sense'):\n",
    "        res[sense['s']] = {'full_text':[],'bingo_sentence':[]}\n",
    "        for instance in sense.select('instance'):\n",
    "            full_text = instance.string.strip()\n",
    "            res[sense['s']]['full_text'].append(full_text)\n",
    "            for sentence in hanlp.utils.rules.split_sentence(full_text):\n",
    "                if word in sentence:\n",
    "                    res[sense['s']]['bingo_sentence'].append(sentence)\n",
    "                    break\n",
    "    corpus_labeled[word]=res\n",
    "\n",
    "corpus_labeled_meta = {}\n",
    "for target in corpus_labeled:\n",
    "    num_sense = len(corpus_labeled[target].keys())\n",
    "    senses = []\n",
    "    num_sentense_of_each_sense = {}\n",
    "    for sense in corpus_labeled[target].keys():\n",
    "        num_sentense_of_each_sense[sense] = len(corpus_labeled[target][sense]['full_text'])\n",
    "        senses.append(sense) \n",
    "    corpus_labeled_meta[target] = {'num_sense':num_sense,'senses':senses,'num_sentense_of_each_sense':num_sentense_of_each_sense}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_labeled_parsed = {}\n",
    "for target in corpus_labeled:\n",
    "    corpus_labeled_parsed[target] = []\n",
    "    for sense in corpus_labeled[target]:\n",
    "        for sentence in corpus_labeled[target][sense]['bingo_sentence']:\n",
    "            parse_result = parse(sentence,target)\n",
    "            parse_result['sense'] = sense\n",
    "            corpus_labeled_parsed[target].append(parse_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加id，以及flat\n",
    "for target, sentences in corpus_labeled_parsed.items():\n",
    "    for index, s in enumerate(sentences):\n",
    "        s['id'] = target+'_'+s['sense']+'_'+str(index)\n",
    "\n",
    "corpus_labeled_flat = {}\n",
    "for target, sentences in corpus_labeled_parsed.items():\n",
    "    for index, s in enumerate(sentences):\n",
    "        corpus_labeled_flat[s['id']] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/CLP2010/parsed.json','w') as f:\n",
    "    json.dump(corpus_labeled_parsed,f,ensure_ascii=False)\n",
    "with open('./data/CLP2010/meta.json','w') as f:\n",
    "    json.dump(corpus_labeled_meta,f,ensure_ascii=False)\n",
    "with open('./data/CLP2010/corpus.json','w') as f:\n",
    "    json.dump(corpus_labeled,f,ensure_ascii=False)\n",
    "with open('./data/CLP2010/flat.json','w') as f:\n",
    "    json.dump(corpus_labeled_flat,f,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target in corpus_labeled:\n",
    "    for sense in corpus_labeled[target]:\n",
    "        for sentence in corpus_labeled[target][sense]['bingo_sentence']:\n",
    "            if len(sentence)>120:\n",
    "                s = process_long_sentence(sentence,target)\n",
    "                if len(s)==0:\n",
    "                    print(sentence,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果及格式\n",
    "**`corpus_labeled_parsed`**: a dict,语料预处理后的结果。将多义词作为key，value是一个列表，每一个item是一个dict，对应一个例句解析后的结果。格式如下：\n",
    "```\n",
    "{\n",
    "    '标兵': [\n",
    "        { # 例句1\n",
    "            sense: 'S0',\n",
    "            is_target_split: False,\n",
    "            sentence: '加快发展,向标兵看齐,目前已经变成大家自觉的行为。',\n",
    "            target: '标兵',\n",
    "            target_index: 4,\n",
    "            tokens: ['发展',...],\n",
    "            pos: ['VV',...],\n",
    "            ne: [{},...],\n",
    "            dp: [\n",
    "                {'index': 0, 'token': '加快', 'pos': 'VV', 'relate': 'conj', 'head': 9},\n",
    "                ...\n",
    "            ]\n",
    "        },\n",
    "        {...}, # 例句2\n",
    "        ...\n",
    "    ],\n",
    "    '东北': [\n",
    "        \n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`corpus_labeled`**：语料，结构：\n",
    "```\n",
    "{\n",
    "    '标兵': {\n",
    "        'S0': {\n",
    "            'full_text': ['xxx,'xx',...],\n",
    "            'bingo_sentence': ['xxx,'xx',...]\n",
    "        },\n",
    "        'S1': {\n",
    "            'full_text': ['xxx,'xx',...],\n",
    "            'bingo_sentence': ['xxx,'xx',...]\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`corpus_labeled_meta`**: 语料元信息\n",
    "```\n",
    "{\n",
    "    '标兵':{'num_sense':2,'senses':['S0','S1'],'num_sentense_of_each_sense':{'S0':18,'S1':32}},\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征词提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_punct(item): \n",
    "    return item['relate']=='punct'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linked_words(sentence_parsed,filter_func=False):\n",
    "    \"\"\"从解析后的句子中提取上下文依存词汇，sentence_parsed格式见corpus_labeled_parsed\n",
    "    return：[{'token': xxx,'relate': xxx, 'is_head': False},...]\n",
    "    \"\"\"\n",
    "    s = sentence_parsed\n",
    "    linked = []\n",
    "    target_index = s['target_index']\n",
    "    for item in s['dp']:\n",
    "        if item['head'] == target_index:\n",
    "            # is_head标明这个关联词在这个依存关系中是不是head\n",
    "            # 当前循环是添加所有以target为head的词，所以添加的这些词的is_head都是false\n",
    "            if filter_func and filter_func(item):\n",
    "                continue\n",
    "            linked.append({**item, 'is_head': False, 'dist': abs(target_index-item['index']), 'type':'dp'})\n",
    "    target_head = s['dp'][target_index]['head']\n",
    "    if target_head>=0:\n",
    "        linked.append({**s['dp'][target_head], 'is_head': True, 'dist': abs(target_index-s['dp'][target_head]['index']), 'type':'dp'})\n",
    "    return linked\n",
    "\n",
    "def get_window_words(sentence_parsed,window=5,filter_func=False):\n",
    "    \"\"\"从解析后的句子中提取上下文window内的词汇，sentence_parsed格式见corpus_labeled_parsed\n",
    "    return：[{'token': xxx,'relate': xxx, 'is_head': False},...]\n",
    "    \"\"\"\n",
    "    s = sentence_parsed\n",
    "    window_words = []\n",
    "    target_index = s['target_index']\n",
    "    for index in range(target_index-window,target_index+window):\n",
    "        if index>-1 and index!=target_index and index<len(s['tokens']):\n",
    "            item = s['dp'][index]\n",
    "            if filter_func and filter_func(item):\n",
    "                continue\n",
    "            window_words.append({**item, 'is_head': False, 'dist': abs(target_index-item['index']), 'type':'window'})\n",
    "    return window_words\n",
    "\n",
    "def get_mixed_words(sentence_parsed,window=5,filter_func=False):\n",
    "    linked_words = get_linked_words(sentence_parsed,filter_func=filter_func)\n",
    "    linked_indexes = [item['index'] for item in linked_words]\n",
    "    window_words = get_window_words(sentence_parsed,window=window,filter_func=filter_func)\n",
    "    mixed_words = linked_words.copy()\n",
    "    for item in window_words:\n",
    "        if item['index'] not in linked_indexes:\n",
    "            mixed_words.append(item)\n",
    "    return mixed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dp and pos weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_pos_dict  = {\n",
    "    'nsubj': 2,\n",
    "     'dobj': 2,\n",
    "     'pobj': 2,\n",
    "     'xsubj': 2,\n",
    "     'nsubjpass': 2,\n",
    "     'nn': 2,\n",
    "     'conj': 2,\n",
    "     'amod': 2,\n",
    "     'rcmod': 2,\n",
    "     'cc': 2,\n",
    "     'dvpmod': 2,\n",
    "    'VA': 1.2, 'VV': 1.2, 'NR': 1.2, 'NT': 1.2, 'NN': 1.2, 'AD': 1.2\n",
    "               }\n",
    "def get_dp_weight(item):\n",
    "    dp_pos = item['type']\n",
    "    dp_type = item['relate']\n",
    "    pos_type = item['pos']\n",
    "    if dp_pos=='dp':\n",
    "        return dp_pos_dict.get(dp_type,1.6)\n",
    "    else:\n",
    "        return dp_pos_dict.get(pos_type,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_weight(contexts, bind_dp=False):\n",
    "    corpus = [' '.join([item['token'] for item in context]) for context in contexts]\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "    X = vectorizer.fit_transform(corpus).toarray()\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    weights = []\n",
    "    for i,context in enumerate(contexts):\n",
    "        weight = []\n",
    "        for item in context:\n",
    "            try:\n",
    "                w = X[i, terms.index(item['token'])]\n",
    "                weight.append(w*get_dp_weight(item) if bind_dp else w)\n",
    "            except:\n",
    "                weight.append(0)\n",
    "        weights.append(weight)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_weight(contexts, bind_dp=False):\n",
    "    counter = Counter()\n",
    "    total_tf = 0\n",
    "    for c in contexts:\n",
    "        counter.update([item['token'] for item in c])\n",
    "        total_tf+=len(c)\n",
    "    weights = []\n",
    "    for i,context in enumerate(contexts):\n",
    "        weight = []\n",
    "        for item in context:\n",
    "            w = counter[item['token']]/total_tf\n",
    "            weight.append((w*get_dp_weight(item) if bind_dp else w))\n",
    "        weights.append(weight)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf(window=5):\n",
    "    counter = Counter()\n",
    "    for sid,s in corpus_labeled_flat.items():\n",
    "        context_items = get_mixed_words(s,window=window,filter_func=filter_punct)\n",
    "        context_words = [word['token'] for word in context_items]\n",
    "        words_bind_target = [f'{word}-{s[\"target\"]}' for word in context_words]\n",
    "        counter.update(context_words)\n",
    "        counter.update(words_bind_target)\n",
    "        counter.update([s[\"target\"]])\n",
    "    return counter\n",
    "\n",
    "tf_dict = {}\n",
    "\n",
    "def get_pmi_weight(contexts,target,window=5, bind_dp=False):\n",
    "    if window in tf_dict:\n",
    "        tf_base = tf_dict[window]\n",
    "    else:\n",
    "        tf_base = get_tf(window=window)\n",
    "        tf_dict[window] = tf_base\n",
    "\n",
    "    tf_total = sum(tf_base.values())\n",
    "    \n",
    "    weights = []\n",
    "    for i,context in enumerate(contexts):\n",
    "        weight = []\n",
    "        for item in context:\n",
    "            token = item['token']\n",
    "            bind_tf = tf_base[f'{token}-{target}']\n",
    "            w = np.log2(bind_tf*tf_total/tf_base[token]/tf_base[target])\n",
    "            weight.append((w*get_dp_weight(item) if bind_dp else w))\n",
    "        weights.append(weight)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算上下文向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练好的embedding\n",
    "with open('/Users/zeason/usr/data/WSI/news300d') as f:\n",
    "    wv = KeyedVectors.load_word2vec_format(f, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_vec(words,weights):\n",
    "    \"\"\"计算上下文词向量，words格式[{'token': xxx,'relate': xxx, 'is_head': False}, ...]\n",
    "    \"\"\"\n",
    "    bingo = []\n",
    "    missed = []\n",
    "    if(len(words)==0):\n",
    "        return np.zeros(300)\n",
    "    context = np.zeros(300)\n",
    "    for word, weight in zip(words,weights):\n",
    "        if word['token'] in wv:\n",
    "            bingo.append(word['token'])\n",
    "            context += wv.get_vector(word['token'])*weight\n",
    "        else:\n",
    "            missed.append(word['token'])\n",
    "    return context/len(words), bingo, missed\n",
    "\n",
    "# 得到上下文向量化表示\n",
    "def get_vecs(target,context_type='dp',window=5,filter_func=filter_punct, weight_type='norm', bind_dp=False):\n",
    "    \"\"\"得到一个target的所有上下文的向量化表示,以及周边信息\"\"\"\n",
    "    vecs = []\n",
    "    bingo = []\n",
    "    missed =[]\n",
    "    labels = []\n",
    "    sentence_ids = []\n",
    "    contexts = []\n",
    "    weights = []\n",
    "    # 上下文词汇\n",
    "    for s in corpus_labeled_parsed[target]:\n",
    "        if s['target_index']>-1:\n",
    "            context_words = []\n",
    "            if context_type=='dp':\n",
    "                context_words = get_linked_words(s,filter_func=filter_func)\n",
    "            if context_type=='window':\n",
    "                context_words = get_window_words(s,window=window,filter_func=filter_func)\n",
    "            if context_type=='mixed':\n",
    "                context_words = get_mixed_words(s,window=window,filter_func=filter_func)\n",
    "            contexts.append(context_words)\n",
    "            labels.append(s['sense'])\n",
    "            sentence_ids.append(s['id'])\n",
    "    # 词项权重        \n",
    "    if weight_type=='norm':\n",
    "        for context in contexts:\n",
    "            weights.append([1]*len(context))\n",
    "    elif weight_type=='tfidf':\n",
    "        weights=get_tfidf_weight(contexts, bind_dp=bind_dp)\n",
    "    elif weight_type=='tf':\n",
    "        weights=get_tf_weight(contexts, bind_dp=bind_dp)\n",
    "    elif weight_type=='pmi':\n",
    "        weights=get_pmi_weight(contexts,target=target, window=window, bind_dp=bind_dp)\n",
    "     \n",
    "    # 计算综合向量\n",
    "    for context, weight in zip(contexts, weights):\n",
    "        vec, _bingo, _missed = get_context_vec(context, weight)\n",
    "        vecs.append(vec)\n",
    "        bingo.append(_bingo)\n",
    "        missed.append(_missed)\n",
    "    info = {'bingo':bingo,'missed':missed,'ids':sentence_ids, 'contexts': contexts, 'weights':weights}\n",
    "    return vecs, labels, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# 聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## 聚类算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmeans and dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚类得到预测标签,并保存每个簇的句子\n",
    "def km_cluster(X, k=2,**kwargs):\n",
    "    km = KMeans(n_clusters=k, max_iter=400, n_init=15, init='random',**kwargs)\n",
    "    km.fit(X)\n",
    "    return km.labels_, km.inertia_\n",
    "\n",
    "def dbscan(X, eps=0.1, minPts=3):\n",
    "    db =  DBSCAN(eps=eps, min_samples=minPts, algorithm='auto', n_jobs=-1)\n",
    "    db.fit(data)\n",
    "    return db.labels_, None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二次聚类 re-weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_calc_vecs(contexts,weights,pred):\n",
    "    word_freq_dict = {}\n",
    "    word_reweight_dict = {}\n",
    "    new_vecs = []\n",
    "    ens = []\n",
    "    for context, label in zip(contexts, pred):\n",
    "        for item in context:\n",
    "            if item['token'] in word_freq_dict:\n",
    "                if label in word_freq_dict[item['token']]:\n",
    "                    word_freq_dict[item['token']][label]+=1\n",
    "                else:\n",
    "                    word_freq_dict[item['token']][label]=1\n",
    "            else:\n",
    "                word_freq_dict[item['token']]={label:1,'total':0} \n",
    "            word_freq_dict[item['token']]['total'] +=1\n",
    "    for token, freq in word_freq_dict.items():\n",
    "        entropy = 0\n",
    "        for c,f in freq.items():\n",
    "            if c != 'total':\n",
    "                p = f/freq['total']\n",
    "                entropy -= p*np.log2(p)\n",
    "        ens.append(entropy)\n",
    "#         word_reweight_dict[token] = 1/entropy\n",
    "        if entropy<=0.1:\n",
    "            word_reweight_dict[token] = 4\n",
    "        elif entropy<=0.5:\n",
    "            word_reweight_dict[token] = 3\n",
    "        elif entropy<=0.8:\n",
    "            word_reweight_dict[token] = 2\n",
    "        else:\n",
    "            word_reweight_dict[token] = 1\n",
    "    for context, weight in zip(contexts, weights):\n",
    "        new_weight = []\n",
    "        for item, item_weight in zip(context, weight):\n",
    "            new_item_weight = item_weight*word_reweight_dict[item['token']]\n",
    "            new_weight.append(new_item_weight)\n",
    "\n",
    "        new_vec = get_context_vec(context, new_weight)[0]\n",
    "        new_vecs.append(new_vec)\n",
    "    \n",
    "    return new_vecs, ens  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 确定k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box(data):\n",
    "    dim = data.shape[1]\n",
    "    boxes = []\n",
    "    for i in range(dim):\n",
    "        data_min = np.amin(data[:, i])\n",
    "        data_max = np.amax(data[:, i])\n",
    "        boxes.append((data_min, data_max))\n",
    "    return boxes\n",
    "\n",
    "def best_k_gap(data, max_K, B, cluster_algorithm,thre=0.05, **kwargs):\n",
    "    num_points, dim = data.shape\n",
    "    K_range = np.arange(1, max_K, dtype=int)\n",
    "    num_K = len(K_range)\n",
    "    boxes = bounding_box(data)\n",
    "    data_generate = np.zeros((num_points, dim))\n",
    "    log_Wks = np.zeros(num_K)\n",
    "    gaps = np.zeros(num_K)\n",
    "    sks = np.zeros(num_K)\n",
    "    for ind_K, K in enumerate(K_range):\n",
    "        _, inertia = cluster_algorithm(data,K,**kwargs)\n",
    "        log_Wks[ind_K] = np.log(inertia)\n",
    "        # generate B reference data sets\n",
    "        log_Wkbs = np.zeros(B)\n",
    "        for b in range(B):\n",
    "            for i in range(num_points):\n",
    "                for j in range(dim):\n",
    "                    data_generate[i][j] = \\\n",
    "                        np.random.uniform(boxes[j][0], boxes[j][1])\n",
    "            _, inertia = cluster_algorithm(data_generate,K,**kwargs)\n",
    "            log_Wkbs[b] = \\\n",
    "                np.log(inertia)\n",
    "        gaps[ind_K] = np.mean(log_Wkbs) - log_Wks[ind_K]\n",
    "        sks[ind_K] = np.std(log_Wkbs) * np.sqrt(1 + 1.0 / B)\n",
    "    \n",
    "    best_k = 1\n",
    "    for i in range(len(gaps)):\n",
    "        if gaps[i+1]/gaps[i] - 1 < thre:\n",
    "            best_k = i+1\n",
    "            break\n",
    "    return best_k\n",
    "\n",
    "def best_k_elbow(data, max_k, cluster_algorithm, thre=0.1, **kwargs):\n",
    "    best_k = 1\n",
    "    for i in range(1, max_k):\n",
    "        _, inertia = cluster_algorithm(data,i,**kwargs)\n",
    "        if i>1 and inertia_pre/inertia-1<thre:\n",
    "            best_k = i\n",
    "            break\n",
    "        inertia_pre =  inertia \n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现CLP2010的F-score计算\n",
    "def eval_f_score(pred, true):\n",
    "    \"\"\"实现CLP2010的F1 Score计算，pred and true: list,不要求用相同的标签去标记类别，标签的作用只是将样本划分而已\"\"\"\n",
    "    n_pred = len(set(pred))\n",
    "    n_true = len(set(true))\n",
    "    f1_crs = []\n",
    "    n_crs = []\n",
    "    for cr in set(true):\n",
    "        f1_cr_si = []\n",
    "        for si in set(pred):\n",
    "            n_si=n_cr=n_cr_si=0\n",
    "            for index,label in enumerate(pred):\n",
    "                if label==si:\n",
    "                    n_si += 1\n",
    "                if true[index]==cr:\n",
    "                    n_cr += 1\n",
    "                if label==si and true[index]==cr:\n",
    "                    n_cr_si += 1\n",
    "            p = n_cr_si/n_si\n",
    "            r = n_cr_si/n_cr\n",
    "            f1 = 2*p*r/(p+r) if p+r>0 else 0\n",
    "            f1_cr_si.append(f1)\n",
    "        f1_crs.append(max(f1_cr_si))\n",
    "        n_crs.append(n_cr)\n",
    "    n_total = sum(n_crs)\n",
    "    f1_score = 0\n",
    "    for f, n in zip(f1_crs,n_crs):\n",
    "        f1_score += (f*n/n_total)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_res(pred, info):\n",
    "    res = {p: [] for p in set(pred)}\n",
    "    for p, sid, bingo, missed in zip(pred,info['ids'],info['bingo'],info['missed']):\n",
    "        res[p].append('%s %s\\t%s' %(sid.ljust(10), process_long_sentence(corpus_labeled_flat[sid]['sentence'],corpus_labeled_flat[sid]['target']), (','.join(bingo) + '|' + ','.join(missed)).ljust(30)))\n",
    "    for p in res:\n",
    "        print(str(p).center(10,'*'))\n",
    "        for row in res[p]:\n",
    "            print(row)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, X, labels, bonus={'trigger':False},pred_ready=False, **kwargs):\n",
    "    \"\"\"\n",
    "        evaluate cluster algorithm or features just on ONE target\n",
    "        \n",
    "        Args:\n",
    "            model: cluster algorithm\n",
    "            X: n*m matrix of features, n: rows， m: # features\n",
    "            labels: list of n labels\n",
    "            bonus: 彩蛋。。。就是把一个函数放进来运行，目前设计来打印结果。\n",
    "    \"\"\"\n",
    "    pred,_ = model(np.array(X), **kwargs) if not pred_ready else pred_ready\n",
    "    # f1 score\n",
    "    f = eval_f_score(pred, labels)\n",
    "    # v score\n",
    "    hs = metrics.homogeneity_score(labels, pred)\n",
    "    cs = metrics.completeness_score(labels, pred)\n",
    "    vs = metrics.v_measure_score(labels, pred)\n",
    "    \n",
    "    if bonus['trigger']:\n",
    "        print('F-score: %0.2f .' % f)\n",
    "        print('v_measure: %0.2f, homogeneity: %0.2f, completeness: %0.2f .' % (vs, hs, cs))\n",
    "        bonus['func'](pred,**bonus['params'])\n",
    "        \n",
    "    return {'F-score': f,'Homogeneity': hs,'Completeness': cs,'V-measure': vs}, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all(model, X_all, labels_all, spec, display=True, **kwargs):\n",
    "    \"\"\"\n",
    "        evaluate cluster algorithm or features just on ALL target\n",
    "        \n",
    "        Args:\n",
    "            model: cluster algorithm\n",
    "            X_all: dict, target as key, value is n*m matrix of features, n: rows， m: # features\n",
    "            labels_all: dict, target as key, value is a list of n labels\n",
    "            spec: dict，target as key，value是给每个target的参数，放到test里\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    for target in X_all:\n",
    "        scores[target], pred = test(model, X_all[target], labels_all[target], **spec[target],**kwargs)\n",
    "        \n",
    "    res =  pd.DataFrame(scores).T.applymap(lambda x: round(x,3))\n",
    "    res = res[['F-score','V-measure','Homogeneity','Completeness']]\n",
    "    res.sort_values('F-score',inplace=True)\n",
    "    if display:\n",
    "        display(res.describe([.25,.5,.75,.9,.95]))\n",
    "        display(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 公共方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car(target,model=km_cluster,**kwargs):\n",
    "    \"\"\"对单个目标词进行测试\"\"\"\n",
    "    vecs, labels, info = get_vecs(target,**kwargs)\n",
    "    bonus = {'func':print_res,'params':{'info': info},'trigger':True}\n",
    "    spec = {'k':corpus_labeled_meta[target]['num_sense'], 'bonus':bonus} \n",
    "    test(model, vecs, labels, **spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_reweight(target,model=km_cluster,**kwargs):\n",
    "    \"\"\"对单个目标词进行词义归纳，并使用二次聚类调整权重\"\"\"\n",
    "    vecs, labels, info = get_vecs(target,**kwargs)\n",
    "    bonus = {'func':print_res,'params':{'info': info},'trigger':True}\n",
    "    spec = {'k':corpus_labeled_meta[target]['num_sense'], 'bonus':bonus} \n",
    "    pre_score, pre_pred = test(model, vecs, labels, **spec)\n",
    "    \n",
    "    # re-weight and re-run\n",
    "    new_vecs, ens = re_calc_vecs(info['contexts'],info['weights'],pre_pred)\n",
    "    score, pred = test(model, vecs, labels, **spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bus(model=kmeans,index='all', k='gold_standard', **kwargs):\n",
    "    \"\"\"运行词义归纳系统，获得评测结果\"\"\"\n",
    "    vecs_all = {}\n",
    "    labels_all = {}\n",
    "    spec = {}\n",
    "    for target in corpus_labeled_meta:\n",
    "        vecs, labels, info = get_vecs(target,**kwargs)\n",
    "        vecs_all[target] = vecs\n",
    "        labels_all[target] = labels\n",
    "        bonus = {'func':print_res,'params':{'info': info},'trigger':False}\n",
    "        if k=='gold_standard':\n",
    "            spec[target] = {'k':corpus_labeled_meta[target]['num_sense'], 'bonus':bonus}\n",
    "        else:\n",
    "            spec[target] = {'k':k[target], 'bonus':bonus}\n",
    "    res_mean = []\n",
    "    for i in range(5):\n",
    "        res = test_all(model,vecs_all,labels_all,spec,display=False)\n",
    "        res_mean.append(res.mean())\n",
    "    final_res = pd.DataFrame(res_mean).mean().to_frame().T\n",
    "    final_res.index = [index]\n",
    "    return final_res.applymap(lambda x: round(x*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上下文特征向量验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征词提取方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>V-measure</th>\n",
       "      <th>Homogeneity</th>\n",
       "      <th>Completeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>窗口上下文（window=5）</th>\n",
       "      <td>72.46</td>\n",
       "      <td>38.36</td>\n",
       "      <td>37.11</td>\n",
       "      <td>40.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>窗口上下文（window=7）</th>\n",
       "      <td>74.22</td>\n",
       "      <td>42.33</td>\n",
       "      <td>41.26</td>\n",
       "      <td>44.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>窗口上下文（window=8）</th>\n",
       "      <td>75.33</td>\n",
       "      <td>42.92</td>\n",
       "      <td>42.15</td>\n",
       "      <td>44.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>窗口上下文（window=10）</th>\n",
       "      <td>75.24</td>\n",
       "      <td>42.75</td>\n",
       "      <td>42.08</td>\n",
       "      <td>43.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>窗口上下文（window=12）</th>\n",
       "      <td>73.87</td>\n",
       "      <td>41.30</td>\n",
       "      <td>40.34</td>\n",
       "      <td>42.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  F-score  V-measure  Homogeneity  Completeness\n",
       "窗口上下文（window=5）     72.46      38.36        37.11         40.31\n",
       "窗口上下文（window=7）     74.22      42.33        41.26         44.03\n",
       "窗口上下文（window=8）     75.33      42.92        42.15         44.16\n",
       "窗口上下文（window=10）    75.24      42.75        42.08         43.93\n",
       "窗口上下文（window=12）    73.87      41.30        40.34         42.72"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_ext = []\n",
    "for window in (5,7,8,10,12):\n",
    "    res = bus(model=km_cluster,index=f'窗口上下文（window={i}）',context_type='window', window=window, weight_type='norm')\n",
    "    scores_ext.append(res)\n",
    "pd.concat(scores_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>V-measure</th>\n",
       "      <th>Homogeneity</th>\n",
       "      <th>Completeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>依存句法上下文</th>\n",
       "      <td>71.61</td>\n",
       "      <td>29.24</td>\n",
       "      <td>26.25</td>\n",
       "      <td>35.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         F-score  V-measure  Homogeneity  Completeness\n",
       "依存句法上下文    71.61      29.24        26.25         35.01"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_dp = bus(model=km_cluster,index=f'依存句法上下文',context_type='dp', window=8, weight_type='norm')\n",
    "scores_dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>V-measure</th>\n",
       "      <th>Homogeneity</th>\n",
       "      <th>Completeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>混合上下文（window=5）</th>\n",
       "      <td>74.53</td>\n",
       "      <td>42.29</td>\n",
       "      <td>40.91</td>\n",
       "      <td>43.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>混合上下文（window=7）</th>\n",
       "      <td>75.32</td>\n",
       "      <td>42.86</td>\n",
       "      <td>41.65</td>\n",
       "      <td>44.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>混合上下文（window=8）</th>\n",
       "      <td>76.15</td>\n",
       "      <td>43.58</td>\n",
       "      <td>42.59</td>\n",
       "      <td>44.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>混合上下文（window=10）</th>\n",
       "      <td>75.98</td>\n",
       "      <td>43.81</td>\n",
       "      <td>42.81</td>\n",
       "      <td>44.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>混合上下文（window=12）</th>\n",
       "      <td>75.71</td>\n",
       "      <td>43.04</td>\n",
       "      <td>42.10</td>\n",
       "      <td>44.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  F-score  V-measure  Homogeneity  Completeness\n",
       "混合上下文（window=5）     74.53      42.29        40.91         43.76\n",
       "混合上下文（window=7）     75.32      42.86        41.65         44.15\n",
       "混合上下文（window=8）     76.15      43.58        42.59         44.62\n",
       "混合上下文（window=10）    75.98      43.81        42.81         44.85\n",
       "混合上下文（window=12）    75.71      43.04        42.10         44.02"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_mixed = []\n",
    "for window in (5,7,8,10,12):\n",
    "    res = bus(model=km_cluster,index=f'混合上下文（window={window}）',context_type='mixed', window=window, weight_type='norm')\n",
    "    scores_mixed.append(res)\n",
    "pd.concat(scores_mixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征词权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>V-measure</th>\n",
       "      <th>Homogeneity</th>\n",
       "      <th>Completeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>句法特征*词频</th>\n",
       "      <td>68.20</td>\n",
       "      <td>20.60</td>\n",
       "      <td>19.89</td>\n",
       "      <td>21.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>句法特征*pmi</th>\n",
       "      <td>78.48</td>\n",
       "      <td>47.79</td>\n",
       "      <td>45.88</td>\n",
       "      <td>49.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          F-score  V-measure  Homogeneity  Completeness\n",
       "句法特征*词频     68.20      20.60        19.89         21.36\n",
       "句法特征*pmi    78.48      47.79        45.88         49.87"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_weight = []\n",
    "for weight_type in [('tf','句法特征*词频'),('pmi','句法特征*pmi')]:\n",
    "    res = bus(model=km_cluster,index=f'{weight_type[1]}',context_type='mixed', window=8, weight_type=weight_type[0], bind_dp=True)\n",
    "    scores_weight.append(res)\n",
    "df_weight = pd.concat(scores_weight)\n",
    "df_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 改进 k-means 验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始中心和噪声对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>V-measure</th>\n",
       "      <th>Homogeneity</th>\n",
       "      <th>Completeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>改进的k-means</th>\n",
       "      <td>79.42</td>\n",
       "      <td>47.42</td>\n",
       "      <td>46.55</td>\n",
       "      <td>48.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            F-score  V-measure  Homogeneity  Completeness\n",
       "改进的k-means    79.42      47.42        46.55         48.33"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 运行前需要修改sklearn源码\n",
    "df_cz = bus(model=km_cluster,index='改进的k-means',context_type='mixed', window=8, weight_type='pmi', bind_dp=True)\n",
    "df_cz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>V-measure</th>\n",
       "      <th>Homogeneity</th>\n",
       "      <th>Completeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dbscan</th>\n",
       "      <td>67.07</td>\n",
       "      <td>34.42</td>\n",
       "      <td>33.4</td>\n",
       "      <td>35.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        F-score  V-measure  Homogeneity  Completeness\n",
       "dbscan    67.07      34.42         33.4         35.51"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_db = bus(model=dbscan, index='dbscan',context_type='mixed', window=8, weight_type='pmi', bind_dp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 确定k值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标兵: 2, gap: 3\n",
      "扼杀: 2, gap: 3\n",
      "东西: 3, gap: 3\n",
      "补贴: 2, gap: 2\n",
      "调动: 3, gap: 3\n",
      "出口: 3, gap: 3\n",
      "戳穿: 2, gap: 3\n",
      "澄清: 2, gap: 4\n",
      "材料: 3, gap: 3\n",
      "春秋: 3, gap: 3\n",
      "大人: 2, gap: 3\n",
      "断交: 2, gap: 4\n",
      "打: 21, gap: 2\n",
      "反射: 2, gap: 3\n",
      "程序: 2, gap: 2\n",
      "吃饭: 2, gap: 3\n",
      "动力: 2, gap: 3\n",
      "东北: 2, gap: 3\n",
      "初二: 2, gap: 3\n",
      "发动: 3, gap: 3\n",
      "断气: 2, gap: 2\n",
      "冲洗: 2, gap: 4\n",
      "打开: 3, gap: 4\n",
      "保安: 2, gap: 3\n",
      "单纯: 2, gap: 3\n",
      "把握: 4, gap: 3\n",
      "翻身: 2, gap: 3\n",
      "打断: 2, gap: 3\n",
      "导师: 2, gap: 3\n",
      "采购: 2, gap: 3\n",
      "报销: 2, gap: 3\n",
      "保管: 3, gap: 3\n",
      "打气: 2, gap: 2\n",
      "草包: 2, gap: 3\n",
      "大军: 2, gap: 3\n",
      "打破: 2, gap: 3\n",
      "冲撞: 2, gap: 3\n",
      "便宜: 3, gap: 3\n",
      "参加: 2, gap: 3\n",
      "充电: 2, gap: 3\n",
      "病毒: 2, gap: 3\n",
      "背离: 2, gap: 3\n",
      "发展: 2, gap: 2\n",
      "东方: 2, gap: 3\n",
      "杜鹃: 2, gap: 4\n",
      "哺育: 2, gap: 3\n",
      "比重: 2, gap: 3\n",
      "大气: 2, gap: 3\n",
      "暗淡: 2, gap: 3\n",
      "大陆: 2, gap: 3\n"
     ]
    }
   ],
   "source": [
    "k_gap = []\n",
    "for target in corpus_labeled_meta:\n",
    "    vecs, labels, info = get_vecs('东北',context_type='window', window=8, weight_type='pmi',filter_func=filter_punct)\n",
    "    k = best_k_gap(np.array(vecs), 10, 50, km_cluster)\n",
    "    k_gap[target] = k\n",
    "    print(f'{target}: {corpus_labeled_meta[target][\"num_sense\"]}, gap: {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>V-measure</th>\n",
       "      <th>Homogeneity</th>\n",
       "      <th>Completeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gap Statistic</th>\n",
       "      <td>78.79</td>\n",
       "      <td>47.04</td>\n",
       "      <td>46.32</td>\n",
       "      <td>48.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               F-score  V-measure  Homogeneity  Completeness\n",
       "Gap Statistic    78.79      47.04        46.32         48.33"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gap = bus(model=km_cluster,k=k_gap, index='Gap Statistic',context_type='mixed', window=8, weight_type='pmi', bind_dp=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二次聚类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F-score         0.79422\n",
      "V-measure       0.45608\n",
      "Homogeneity     0.44442\n",
      "Completeness    0.47860\n",
      "dtype: float64\n",
      "F-score         0.80110\n",
      "V-measure       0.47670\n",
      "Homogeneity     0.46670\n",
      "Completeness    0.49702\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_scores = {}\n",
    "scores = {}\n",
    "for target in corpus_labeled_meta:\n",
    "    vecs, labels, info = get_vecs(target, context_type='mixed', window=15, weight_type='pmi')\n",
    "    bonus = {'func':print_res,'params':{'info': info},'trigger':False}\n",
    "    spec = {'k':corpus_labeled_meta[target]['num_sense'], 'bonus':bonus} \n",
    "    \n",
    "    pre_pack = []\n",
    "    pack = []\n",
    "    for i in range(10):\n",
    "        pre_score, pre_pred = test(km_cluster, vecs, labels, **spec)\n",
    "\n",
    "        # re-weight and re-run\n",
    "        new_vecs, ens = re_calc_vecs(info['contexts'],info['weights'],pre_pred)\n",
    "        score, pred = test(km_cluster, new_vecs, labels, **spec)\n",
    "        \n",
    "        pre_pack.append(pre_score)\n",
    "        pack.append(score)\n",
    "    pre_scores[target] = pd.DataFrame(pre_pack).mean()\n",
    "    scores[target] = pd.DataFrame(pack).mean()\n",
    "pre_res =  pd.DataFrame(pre_scores).T.applymap(lambda x: round(x,3))\n",
    "pre_res = pre_res[['F-score','V-measure','Homogeneity','Completeness']]\n",
    "res =  pd.DataFrame(scores).T.applymap(lambda x: round(x,3))\n",
    "res = res[['F-score','V-measure','Homogeneity','Completeness']]\n",
    "print(pre_res.mean())\n",
    "print(res.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-score: 0.80 .\n",
      "v_measure: 0.29, homogeneity: 0.29, completeness: 0.29 .\n",
      "****0*****\n",
      "程序_S0_3    该出让符合法定出让程序。\t以,元,挂牌,该,出让,符合,法定,出让|1314万    \n",
      "程序_S0_4    进一步规范了此类案件的审理程序。\t由此,进一步,规范,了,此,类,案件,的,审理|      \n",
      "程序_S0_5    占有优势的股东有时可能强行启动临时会议程序。\t占有,优势,的,股东,有时,可能,强行,启动,临时,会议| \n",
      "程序_S0_6    引咎辞职官员是可以依据宪法和法律所规定的程序重新担任公职的，\t官员,是,可以,依据,宪法,和,法律,所,规定,的,重新,担任,公职,的,其,基本,依据,就|\n",
      "程序_S0_7    公司股东会会议的召集应遵守并执行相应的程序。\t股东会,会议,的,召集,应,遵守,并,执行,相应,的|   \n",
      "程序_S0_8    按法定程序办事，\t温家宝,指出,按,法定,办事,是,依法,行政,的,重要,内容|\n",
      "程序_S0_9    其中强调要“加强行政程序的制度建设”，\t其中,强调,要,加强,行政,的,制度,建设,这,一,点,引|\n",
      "程序_S0_11   它们共同构成了作为税收征纳程序内容的征纳行为，\t纳税,行为,它们,共同,构成,了,作为,税收,征纳,内容,的,征纳,行为,保证,了,税收,这|\n",
      "程序_S0_12   组织审核评议等程序。\t展示,公众,投票,评选,组织,审核,评议,等|       \n",
      "程序_S0_13   基金管理人在履行适当程序后，\t投资,的,其他,品种,基金,管理人,在,履行,适当,后,可以,将,其,纳入,投资,范围|\n",
      "程序_S0_14   最后只有一块土地进入竞价程序，\t元,最后,只,有,一,块,土地,进入,竞价,11,块,以,底价,成交|\n",
      "程序_S0_18   第二步启动38个总行部门副总经理职务公开竞聘程序，\t启动,38,个,总行,部门,副,总经理,职务,公开,竞聘,部门,副总,的,竞聘,工作,将,于,4|\n",
      "程序_S0_19   我们对情况表实施了包括核对、询问、抽查会计记录等我们认为必要的工作程序。\t抽查,会计,记录,等,我们,认为,必要,的,工作|     \n",
      "程序_S0_20   眉山市政务服务中心规划和建设局窗口进一步简化办事程序，\t政务,服务,中心,规划,和,建设局,窗口,进一步,简化,办事,提高,服务,质量,获,好评|\n",
      "程序_S0_21   法律程序是以和平理性的方式解决社会矛盾的最佳途径。\t法律,是,以,和平,理性,的,方式,解决,社会,矛盾|   \n",
      "程序_S0_22   进一步简化程序。\t市民,对,方,测温,的,意见,进一步,简化|第三      \n",
      "程序_S0_23   还要公开预算的编制程序和编制过程。\t的,执行,情况,还,要,公开,预算,的,编制,和,编制,过程|\n",
      "程序_S0_24   汉唐证券有限责任公司于2007年12月26日进入破产清算程序。\t于,2007,年,12,月,26,日,进入,破产,清算|  \n",
      "程序_S1_26   程序（program）是为实现特定目标或解决特定问题而用计算机语言编写的命令序列的集合。\t是,为,实现,特定,目标,或|program        \n",
      "程序_S1_28   作为代码程序员，\t作为,代码,员,必须,将,的,工作,时间,写|30％    \n",
      "程序_S1_36   360公司安全中心宣布将公司的软件程序源代码公开交给国家级的评测机构，\t360,公司,安全,中心,宣布,将,公司,的,软件,源,代码,公开,交给,国家级,的,评测,机构|\n",
      "\n",
      "****1*****\n",
      "程序_S0_0    程序就是整治细节最好的工具。\t就,是,整治,细节,最,好,的,工具|           \n",
      "程序_S0_1    因为有了规范的办事程序，\t因为,有,了,规范,的,办事,现在,我们,这些,平民,百姓,到,政府,机关|\n",
      "程序_S0_2    南宁见义勇为的英雄鲍光蛇在医院冷酷的程序中流尽了最后一滴血。\t南宁,见义勇为,的,英雄,在,医院,冷酷,的,中,流尽,了,最后,一,滴,血|鲍光蛇\n",
      "程序_S0_10   他们虽然嘴上也说“走程序”的必要，\t他们,虽然,嘴上,也,说,走,的,必要,但,在,具体,行动,上|\n",
      "程序_S0_15   只要程序公正，\t只要,公正,官员,并不,需要,问,出处|          \n",
      "程序_S0_16   又符合程序，\t只要,雷,连鸣,有,真,本事,又,符合,未尝,不,可|   \n",
      "程序_S0_17   那么对集体土地上建筑物拆迁程序的缺失则是起到了推波助澜的作用。\t实体,因素,那么,对,集体,土地,上,建筑物,拆迁,的,缺失,则,是,起到,了,推波助澜,的,作用|\n",
      "程序_S1_25   程序是由序列组成的，\t是,由,序列,组成,的,告诉,计算机,如何|        \n",
      "程序_S1_27   为了一个程序运行，\t为了,一,个,运行,计算机,加载,代码,可能,还,要|   \n",
      "程序_S1_29   计算机的程序是有一系列的机器指令组成的，\t计算机,的,是,有,一系列,的,机器,指令,组成,的|   \n",
      "程序_S1_30   是微程序级的命令，\t是,微,的,命令,它,属于,硬件|微指令          \n",
      "程序_S1_31   简称OS）是一管理电脑硬件与软件资源的程序，\t是,一,管理,电脑,硬件,与,软件,资源,的,同时,也,是,计算机,系统,的,内核,与|\n",
      "程序_S1_32   都会有一款标准软件应用程序或工具的一个更新或补丁出现。\t天,都,会,有,一,款,标准,软件,应用,或,工具,的,一,个,更新,或,补丁,出现|\n",
      "程序_S1_33   源代码也称源程序，\t据,了解,源代码,也,称,源,是,一,款,软件,的,最,原始,表现|\n",
      "程序_S1_34   很多情况下人在使用新语言编程序的时候，\t很多,情况,下,人,在,使用,新,语言,编,的,时候,会,采用,老,语言,的,语法|\n",
      "程序_S1_35   有时很难快速找到目标程序。\t多,的,时候,有时,很,难,快速,找到,目标|       \n",
      "程序_S1_37   例如Cowpotato可以在仿真程序上顺利运行，\t解释,说,例如,可以,在,仿真,上,顺利,运行,但是,在,摩托罗拉,的|Pick,Cowpotato,Droid\n",
      "程序_S1_38   我们该采取什么措施来禁止病毒或木马利用应用程序漏洞来对服务器进行非法攻击呢？\t采取,什么,措施,来,禁止,病毒,或,木马,利用,应用,漏洞,来,对,服务器,进行,非法,攻击,呢|\n",
      "程序_S1_39   暴雪官方论坛一则新蓝帖向我们公布了一款新的黑客程序，\t向,我们,公布,了,一,款,新,的,黑客,虽然,看起来,十分,陈旧,的,手法,不过|新蓝帖\n",
      "程序_S1_40   它的产生主要是由于程序对用户输入的数据没有进行细致的过滤，\t它,的,产生,主要,是,由于,对,用户,输入,的,数据,没有,进行,细致,的|\n",
      "程序_S1_41   通过该程序可以翻译动物的语言，\t的,for,应用,通过,该,可以,翻译,动物,的,语言,这样,的话,方便|Translate,animals\n",
      "程序_S1_42   IntelliTrace会收集在调试过程中程序的运行状态，\t会,收集,在,调试,过程,中,的,运行,状态,这些,状态,存到,哪儿|IntelliTrace\n",
      "程序_S1_43   另外一个程序开发的变动是支持原生客户端（Native Client），\t另外,一,个,开发,的,变动,是,支持,原生,客户端|Native\n",
      "程序_S1_44   程序员可以有更多可以做的事情。\t透过,一些,新,的,浏览器,介面,员,可以,有,更,多,可以,做,的,事情|\n",
      "程序_S1_45   在Windows里可以自由添加程序，\t在,Windows,里,可以,自由,添加,它,可以,浏览,网络,中,的,计算机,和|\n",
      "程序_S1_46   这些补丁只能用于修补电脑程序的已有的固定错误。\t这些,补丁,只,能,用于,修补,电脑,的,已,有,的,固定,错误|\n",
      "程序_S1_47   易路联动正是通过技术处理把许多程序优化、变小，\t联动,正,是,通过,技术,处理,把,许多,优化,变小,使,它,能够,装到,很多|易路\n",
      "程序_S1_48   另外这款手机具备JAVA程序扩展能力，\t另外,这,款,手机,具备,扩展,能力,虽然,机身,仅仅,提供,了,11|JAVA\n",
      "程序_S1_49   并发和并行程序通常还只是并行计算专业人士和高端用户才关心的技术，\t在,过去,并发,和,并行,通常,还,只是,并行,计算,专业,人士,和,高端|\n",
      "\n",
      "F-score: 0.84 .\n",
      "v_measure: 0.39, homogeneity: 0.38, completeness: 0.39 .\n",
      "****0*****\n",
      "程序_S0_0    程序就是整治细节最好的工具。\t就,是,整治,细节,最,好,的,工具|           \n",
      "程序_S0_2    南宁见义勇为的英雄鲍光蛇在医院冷酷的程序中流尽了最后一滴血。\t南宁,见义勇为,的,英雄,在,医院,冷酷,的,中,流尽,了,最后,一,滴,血|鲍光蛇\n",
      "程序_S0_10   他们虽然嘴上也说“走程序”的必要，\t他们,虽然,嘴上,也,说,走,的,必要,但,在,具体,行动,上|\n",
      "程序_S0_14   最后只有一块土地进入竞价程序，\t元,最后,只,有,一,块,土地,进入,竞价,11,块,以,底价,成交|\n",
      "程序_S0_16   又符合程序，\t只要,雷,连鸣,有,真,本事,又,符合,未尝,不,可|   \n",
      "程序_S0_17   那么对集体土地上建筑物拆迁程序的缺失则是起到了推波助澜的作用。\t实体,因素,那么,对,集体,土地,上,建筑物,拆迁,的,缺失,则,是,起到,了,推波助澜,的,作用|\n",
      "程序_S1_25   程序是由序列组成的，\t是,由,序列,组成,的,告诉,计算机,如何|        \n",
      "程序_S1_26   程序（program）是为实现特定目标或解决特定问题而用计算机语言编写的命令序列的集合。\t是,为,实现,特定,目标,或|program        \n",
      "程序_S1_27   为了一个程序运行，\t为了,一,个,运行,计算机,加载,代码,可能,还,要|   \n",
      "程序_S1_29   计算机的程序是有一系列的机器指令组成的，\t计算机,的,是,有,一系列,的,机器,指令,组成,的|   \n",
      "程序_S1_30   是微程序级的命令，\t是,微,的,命令,它,属于,硬件|微指令          \n",
      "程序_S1_31   简称OS）是一管理电脑硬件与软件资源的程序，\t是,一,管理,电脑,硬件,与,软件,资源,的,同时,也,是,计算机,系统,的,内核,与|\n",
      "程序_S1_32   都会有一款标准软件应用程序或工具的一个更新或补丁出现。\t天,都,会,有,一,款,标准,软件,应用,或,工具,的,一,个,更新,或,补丁,出现|\n",
      "程序_S1_33   源代码也称源程序，\t据,了解,源代码,也,称,源,是,一,款,软件,的,最,原始,表现|\n",
      "程序_S1_34   很多情况下人在使用新语言编程序的时候，\t很多,情况,下,人,在,使用,新,语言,编,的,时候,会,采用,老,语言,的,语法|\n",
      "程序_S1_35   有时很难快速找到目标程序。\t多,的,时候,有时,很,难,快速,找到,目标|       \n",
      "程序_S1_37   例如Cowpotato可以在仿真程序上顺利运行，\t解释,说,例如,可以,在,仿真,上,顺利,运行,但是,在,摩托罗拉,的|Pick,Cowpotato,Droid\n",
      "程序_S1_38   我们该采取什么措施来禁止病毒或木马利用应用程序漏洞来对服务器进行非法攻击呢？\t采取,什么,措施,来,禁止,病毒,或,木马,利用,应用,漏洞,来,对,服务器,进行,非法,攻击,呢|\n",
      "程序_S1_39   暴雪官方论坛一则新蓝帖向我们公布了一款新的黑客程序，\t向,我们,公布,了,一,款,新,的,黑客,虽然,看起来,十分,陈旧,的,手法,不过|新蓝帖\n",
      "程序_S1_40   它的产生主要是由于程序对用户输入的数据没有进行细致的过滤，\t它,的,产生,主要,是,由于,对,用户,输入,的,数据,没有,进行,细致,的|\n",
      "程序_S1_41   通过该程序可以翻译动物的语言，\t的,for,应用,通过,该,可以,翻译,动物,的,语言,这样,的话,方便|Translate,animals\n",
      "程序_S1_42   IntelliTrace会收集在调试过程中程序的运行状态，\t会,收集,在,调试,过程,中,的,运行,状态,这些,状态,存到,哪儿|IntelliTrace\n",
      "程序_S1_43   另外一个程序开发的变动是支持原生客户端（Native Client），\t另外,一,个,开发,的,变动,是,支持,原生,客户端|Native\n",
      "程序_S1_44   程序员可以有更多可以做的事情。\t透过,一些,新,的,浏览器,介面,员,可以,有,更,多,可以,做,的,事情|\n",
      "程序_S1_45   在Windows里可以自由添加程序，\t在,Windows,里,可以,自由,添加,它,可以,浏览,网络,中,的,计算机,和|\n",
      "程序_S1_46   这些补丁只能用于修补电脑程序的已有的固定错误。\t这些,补丁,只,能,用于,修补,电脑,的,已,有,的,固定,错误|\n",
      "程序_S1_47   易路联动正是通过技术处理把许多程序优化、变小，\t联动,正,是,通过,技术,处理,把,许多,优化,变小,使,它,能够,装到,很多|易路\n",
      "程序_S1_48   另外这款手机具备JAVA程序扩展能力，\t另外,这,款,手机,具备,扩展,能力,虽然,机身,仅仅,提供,了,11|JAVA\n",
      "程序_S1_49   并发和并行程序通常还只是并行计算专业人士和高端用户才关心的技术，\t在,过去,并发,和,并行,通常,还,只是,并行,计算,专业,人士,和,高端|\n",
      "\n",
      "****1*****\n",
      "程序_S0_1    因为有了规范的办事程序，\t因为,有,了,规范,的,办事,现在,我们,这些,平民,百姓,到,政府,机关|\n",
      "程序_S0_3    该出让符合法定出让程序。\t以,元,挂牌,该,出让,符合,法定,出让|1314万    \n",
      "程序_S0_4    进一步规范了此类案件的审理程序。\t由此,进一步,规范,了,此,类,案件,的,审理|      \n",
      "程序_S0_5    占有优势的股东有时可能强行启动临时会议程序。\t占有,优势,的,股东,有时,可能,强行,启动,临时,会议| \n",
      "程序_S0_6    引咎辞职官员是可以依据宪法和法律所规定的程序重新担任公职的，\t官员,是,可以,依据,宪法,和,法律,所,规定,的,重新,担任,公职,的,其,基本,依据,就|\n",
      "程序_S0_7    公司股东会会议的召集应遵守并执行相应的程序。\t股东会,会议,的,召集,应,遵守,并,执行,相应,的|   \n",
      "程序_S0_8    按法定程序办事，\t温家宝,指出,按,法定,办事,是,依法,行政,的,重要,内容|\n",
      "程序_S0_9    其中强调要“加强行政程序的制度建设”，\t其中,强调,要,加强,行政,的,制度,建设,这,一,点,引|\n",
      "程序_S0_11   它们共同构成了作为税收征纳程序内容的征纳行为，\t纳税,行为,它们,共同,构成,了,作为,税收,征纳,内容,的,征纳,行为,保证,了,税收,这|\n",
      "程序_S0_12   组织审核评议等程序。\t展示,公众,投票,评选,组织,审核,评议,等|       \n",
      "程序_S0_13   基金管理人在履行适当程序后，\t投资,的,其他,品种,基金,管理人,在,履行,适当,后,可以,将,其,纳入,投资,范围|\n",
      "程序_S0_15   只要程序公正，\t只要,公正,官员,并不,需要,问,出处|          \n",
      "程序_S0_18   第二步启动38个总行部门副总经理职务公开竞聘程序，\t启动,38,个,总行,部门,副,总经理,职务,公开,竞聘,部门,副总,的,竞聘,工作,将,于,4|\n",
      "程序_S0_19   我们对情况表实施了包括核对、询问、抽查会计记录等我们认为必要的工作程序。\t抽查,会计,记录,等,我们,认为,必要,的,工作|     \n",
      "程序_S0_20   眉山市政务服务中心规划和建设局窗口进一步简化办事程序，\t政务,服务,中心,规划,和,建设局,窗口,进一步,简化,办事,提高,服务,质量,获,好评|\n",
      "程序_S0_21   法律程序是以和平理性的方式解决社会矛盾的最佳途径。\t法律,是,以,和平,理性,的,方式,解决,社会,矛盾|   \n",
      "程序_S0_22   进一步简化程序。\t市民,对,方,测温,的,意见,进一步,简化|第三      \n",
      "程序_S0_23   还要公开预算的编制程序和编制过程。\t的,执行,情况,还,要,公开,预算,的,编制,和,编制,过程|\n",
      "程序_S0_24   汉唐证券有限责任公司于2007年12月26日进入破产清算程序。\t于,2007,年,12,月,26,日,进入,破产,清算|  \n",
      "程序_S1_28   作为代码程序员，\t作为,代码,员,必须,将,的,工作,时间,写|30％    \n",
      "程序_S1_36   360公司安全中心宣布将公司的软件程序源代码公开交给国家级的评测机构，\t360,公司,安全,中心,宣布,将,公司,的,软件,源,代码,公开,交给,国家级,的,评测,机构|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "car_reweight('程序',model=km_cluster, context_type='window', window=8, weight_type='pmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大规模词义归纳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6179, 1000, 0.1392951664577239)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dp依存词在宽为5的window外的占比达到20%\n",
    "from collections import Counter\n",
    "counter = Counter()\n",
    "for s in corpus_labeled_flat:\n",
    "    for token in get_linked_words(corpus_labeled_flat[s]):\n",
    "        counter.update([token['dist']])\n",
    "sum_under5=sum_over5=0\n",
    "for k, v in counter.items():\n",
    "    if k<8:\n",
    "        sum_under5+=v\n",
    "    else:\n",
    "        sum_over5+=v\n",
    "sum_under5,sum_over5,sum_over5/(sum_under5+sum_over5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 39, 3: 9, 21: 1, 4: 1})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sense_nums = []\n",
    "c = Counter()\n",
    "for target in corpus_labeled_meta:\n",
    "    sense_nums.append(corpus_labeled_meta[target]['num_sense'])\n",
    "c.update(sense_nums)\n",
    "c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
